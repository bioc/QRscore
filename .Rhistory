mean(test.data$label==svm.predict)
options(na.action='na.pass')
x <- model.matrix(label ~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- train.data$label
# cross validation on train data with lasso
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
lasso.mod <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- model.matrix(label ~., test.data)[,-1]
lasso.probs <- lasso.mod %>% predict(newx = x.test)
lasso.predicted <- ifelse(lasso.probs > 0.5, 1, 0)
# Model accuracy
mean(lasso.predicted == test.data$label)
table(test.data$label,lasso.predicted)
# Split the data into training and test set
set.seed(1)
training.samples <- degree_mats$label %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- degree_mats [training.samples, ]
test.data <- degree_mats [-training.samples, ]
#degree_mats <- ROSE(label ~ ., p=0.4,data = train.data, seed = 8)$data
rpart.mod <- rpart(label~., data = train.data,#degree_mats ,
control = rpart.control(minsplit = 1,
minbucket=1,
maxdepth = 4,
cp=0)
)
rpart.plot(rpart.mod)
rpart.mod$variable.importance
tree.probs <- predict(rpart.mod,test.data)
tree.predict = ifelse(tree.probs > 0.5, 1, 0)
tree.res = table(test.data$label,tree.predict)
mean(test.data$label==tree.predict)
tree.res
roc <- roc(test.data$label,tree.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,title = "rpart without important SNP")
log.mod = glm(label~., family = "binomial", data = train.data)
summary(log.mod)
log.probs = predict(log.mod,newdata = test.data,type = "response")
log.predict <- ifelse(log.probs > 0.5, 1, 0)
log.res = table(test.data$label,log.predict)
mean(test.data$label==log.predict)
log.res
roc <- roc(test.data$label,log.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Logistic Regression without important SNP")
options(na.action='na.pass')
x <- model.matrix(label ~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- train.data$label
# cross validation on train data with lasso
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
lasso.mod <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- model.matrix(label ~., test.data)[,-1]
lasso.probs <- lasso.mod %>% predict(newx = x.test)
lasso.predicted <- ifelse(lasso.probs > 0.5, 1, 0)
# Model accuracy
mean(lasso.predicted == test.data$label)
table(test.data$label,lasso.predicted)
#show coefficients
coef.lasso.logistic = coef(cv.lasso, cv.lasso$lambda.min)
coef.lasso.logistic = data.frame(variables=coef.lasso.logistic@Dimnames[[1]][which(coef.lasso.logistic!=0)],
coefficients = coef.lasso.logistic@x)
coef.lasso.logistic
roc <- roc(test.data$label,lasso.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Penalized Logistic Regression without important SNP")
rf.mod = randomForest(label~. ,train.data)
rf.probs <- predict(rf.mod, newdata = test.data)
rf.predict <- as.numeric(rf.probs>0.5)
table(test.data$label,rf.predict)
mean(test.data$label==rf.predict)
roc <- roc(test.data$label,rf.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Random Forest without important SNP")
#svm takes large memory space and long time, test error is close to 0.306
svm.mod <- svm(as.numeric(label) ~ ., data = train.data) #, kernel ="radial")
svm.predict= as.numeric(predict(svm.mod, test.data)>0.5)
table(test.data$label,svm.predict)
mean(test.data$label==svm.predict)
# Split the data into training and test set
set.seed(1)
training.samples <- degree_mats$label %>%
createDataPartition(p = 0.9, list = FALSE)
train.data  <- degree_mats [training.samples, ]
test.data <- degree_mats [-training.samples, ]
rpart.mod <- rpart(label~., data = train.data,#degree_mats ,
control = rpart.control(minsplit = 1,
minbucket=1,
maxdepth = 4,
cp=0)
)
rpart.plot(rpart.mod)
rpart.mod$variable.importance
tree.probs <- predict(rpart.mod,test.data)
tree.predict = ifelse(tree.probs > 0.5, 1, 0)
tree.res = table(test.data$label,tree.predict)
mean(test.data$label==tree.predict)
tree.res
roc <- roc(test.data$label,tree.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,title = "rpart without important SNP")
log.mod = glm(label~., family = "binomial", data = train.data)
summary(log.mod)
log.probs = predict(log.mod,newdata = test.data,type = "response")
log.predict <- ifelse(log.probs > 0.5, 1, 0)
log.res = table(test.data$label,log.predict)
mean(test.data$label==log.predict)
log.res
roc <- roc(test.data$label,log.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Logistic Regression without important SNP")
options(na.action='na.pass')
x <- model.matrix(label ~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- train.data$label
# cross validation on train data with lasso
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
lasso.mod <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- model.matrix(label ~., test.data)[,-1]
lasso.probs <- lasso.mod %>% predict(newx = x.test)
lasso.predicted <- ifelse(lasso.probs > 0.5, 1, 0)
# Model accuracy
mean(lasso.predicted == test.data$label)
table(test.data$label,lasso.predicted)
#show coefficients
coef.lasso.logistic = coef(cv.lasso, cv.lasso$lambda.min)
coef.lasso.logistic = data.frame(variables=coef.lasso.logistic@Dimnames[[1]][which(coef.lasso.logistic!=0)],
coefficients = coef.lasso.logistic@x)
coef.lasso.logistic
roc <- roc(test.data$label,lasso.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Penalized Logistic Regression without important SNP")
rf.mod = randomForest(label~. ,train.data)
varImpPlot(rf.mod,cex=0.5)
rf.probs <- predict(rf.mod, newdata = test.data)
rf.predict <- as.numeric(rf.probs>0.5)
table(test.data$label,rf.predict)
mean(test.data$label==rf.predict)
roc <- roc(test.data$label,rf.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Random Forest without important SNP")
#svm takes large memory space and long time, test error is close to 0.306
svm.mod <- svm(as.numeric(label) ~ ., data = train.data) #, kernel ="radial")
svm.predict= as.numeric(predict(svm.mod, test.data)>0.5)
table(test.data$label,svm.predict)
mean(test.data$label==svm.predict)
# Split the data into training and test set
set.seed(1)
training.samples <- degree_mats$label %>%
createDataPartition(p = 0.7, list = FALSE)
train.data  <- degree_mats [training.samples, ]
test.data <- degree_mats [-training.samples, ]
#degree_mats <- ROSE(label ~ ., p=0.4,data = train.data, seed = 8)$data
rpart.mod <- rpart(label~., data = train.data,#degree_mats ,
control = rpart.control(minsplit = 1,
minbucket=1,
maxdepth = 4,
cp=0)
)
rpart.plot(rpart.mod)
rpart.mod$variable.importance
tree.probs <- predict(rpart.mod,test.data)
tree.predict = ifelse(tree.probs > 0.5, 1, 0)
tree.res = table(test.data$label,tree.predict)
mean(test.data$label==tree.predict)
tree.res
roc <- roc(test.data$label,tree.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,title = "rpart without important SNP")
# Split the data into training and test set
set.seed(1)
training.samples <- degree_mats$label %>%
createDataPartition(p = 0.85, list = FALSE)
train.data  <- degree_mats [training.samples, ]
test.data <- degree_mats [-training.samples, ]
rpart.mod <- rpart(label~., data = train.data,#degree_mats ,
control = rpart.control(minsplit = 1,
minbucket=1,
maxdepth = 4,
cp=0)
)
rpart.plot(rpart.mod)
rpart.mod$variable.importance
tree.probs <- predict(rpart.mod,test.data)
tree.predict = ifelse(tree.probs > 0.5, 1, 0)
tree.res = table(test.data$label,tree.predict)
mean(test.data$label==tree.predict)
tree.res
roc <- roc(test.data$label,tree.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,title = "rpart without important SNP")
log.mod = glm(label~., family = "binomial", data = train.data)
summary(log.mod)
log.probs = predict(log.mod,newdata = test.data,type = "response")
log.predict <- ifelse(log.probs > 0.5, 1, 0)
log.res = table(test.data$label,log.predict)
mean(test.data$label==log.predict)
log.res
roc <- roc(test.data$label,log.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Logistic Regression without important SNP")
options(na.action='na.pass')
x <- model.matrix(label ~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- train.data$label
# cross validation on train data with lasso
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
lasso.mod <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- model.matrix(label ~., test.data)[,-1]
lasso.probs <- lasso.mod %>% predict(newx = x.test)
lasso.predicted <- ifelse(lasso.probs > 0.5, 1, 0)
# Model accuracy
mean(lasso.predicted == test.data$label)
table(test.data$label,lasso.predicted)
#show coefficients
coef.lasso.logistic = coef(cv.lasso, cv.lasso$lambda.min)
coef.lasso.logistic = data.frame(variables=coef.lasso.logistic@Dimnames[[1]][which(coef.lasso.logistic!=0)],
coefficients = coef.lasso.logistic@x)
coef.lasso.logistic
roc <- roc(test.data$label,lasso.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Penalized Logistic Regression without important SNP")
rf.mod = randomForest(label~. ,train.data)
varImpPlot(rf.mod,cex=0.5)
# Split the data into training and test set
set.seed(123)
training.samples <- degree_mats$label %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- degree_mats [training.samples, ]
test.data <- degree_mats [-training.samples, ]
rpart.mod <- rpart(label~., data = train.data,#degree_mats ,
control = rpart.control(minsplit = 1,
minbucket=1,
maxdepth = 4,
cp=0)
)
rpart.plot(rpart.mod)
rpart.mod$variable.importance
tree.probs <- predict(rpart.mod,test.data)
tree.predict = ifelse(tree.probs > 0.5, 1, 0)
tree.res = table(test.data$label,tree.predict)
mean(test.data$label==tree.predict)
tree.res
roc <- roc(test.data$label,tree.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,title = "rpart without important SNP")
log.mod = glm(label~., family = "binomial", data = train.data)
summary(log.mod)
log.probs = predict(log.mod,newdata = test.data,type = "response")
log.predict <- ifelse(log.probs > 0.5, 1, 0)
log.res = table(test.data$label,log.predict)
mean(test.data$label==log.predict)
log.res
options(na.action='na.pass')
x <- model.matrix(label ~., train.data)[,-1]
# Convert the outcome (class) to a numerical variable
y <- train.data$label
# cross validation on train data with lasso
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
lasso.mod <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- model.matrix(label ~., test.data)[,-1]
lasso.probs <- lasso.mod %>% predict(newx = x.test)
lasso.predicted <- ifelse(lasso.probs > 0.5, 1, 0)
# Model accuracy
mean(lasso.predicted == test.data$label)
table(test.data$label,lasso.predicted)
#show coefficients
coef.lasso.logistic = coef(cv.lasso, cv.lasso$lambda.min)
coef.lasso.logistic = data.frame(variables=coef.lasso.logistic@Dimnames[[1]][which(coef.lasso.logistic!=0)],
coefficients = coef.lasso.logistic@x)
coef.lasso.logistic
roc <- roc(test.data$label,lasso.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Penalized Logistic Regression without important SNP")
rf.mod = randomForest(label~. ,train.data)
rf.mod = randomForest(label~. ,train.data)
rf.probs <- predict(rf.mod, newdata = test.data)
rf.predict <- as.numeric(rf.probs>0.5)
table(test.data$label,rf.predict)
mean(test.data$label==rf.predict)
roc <- roc(test.data$label,rf.probs,
smoothed = TRUE,
# arguments for ci
ci=TRUE, ci.alpha=0.99, stratified=FALSE,
# arguments for plot
plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
print.auc=TRUE, show.thres=TRUE) #creates an object with all sorts of diagnostics including sensitivities and specificities
sens.ci <- ci.se(roc)
plot(sens.ci, type="shape", col="lightblue")#,main = "Random Forest without important SNP")
svm.mod <- svm(as.numeric(label) ~ ., data = train.data) #, kernel ="radial")
svm.predict= as.numeric(predict(svm.mod, test.data)>0.5)
table(test.data$label,svm.predict)
mean(test.data$label==svm.predict)
knitr::opts_chunk$set(echo = TRUE)
head(degree_mats)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(radiant.data)
library(ggforce)
library(e1071)
library(dplyr)
library(glmnet)
library(caret)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(JOUSBoost)
library(pROC)
#library(DMwR)
library(ROSE)
library(corrplot)
degree_mats = read.csv("~/Documents/ligand_receptor/degree_mats_both_smooth_thres_0.95_num_degrees_20.csv")
gene_pairs = degree_mats[,1]
degree_mats = degree_mats[,c(-1,-4)]
head(gene_pairs[!degree_mats$label])
knitr::opts_chunk$set(echo = FALSE)
library(Seurat)
library(dplyr)
library(ggplot2)
library(igraph)
scale(c(rep(1,20),rep(0,560)))
head(scale(c(rep(2,20),rep(0,560))))
head(scale(c(rep(1,10),rep(0,560))))
head(scale(c(rep(1,5),rep(0,560))))
head(scale(c(rep(1,4),rep(0,560))))
scale(c(rep(1,4
library(mochisR)
?mochis.test
package_version(mochis.test)
setwd("/Users/fandingzhou/Documents/mochis/mochisR")
roxygen2::roxygenise()
roxygen2::roxygenise()
library(mochisR)
?mochis.test
roxygen2::roxygenise()
devtools::document()
library(mochisR)
library(mochisR)
?mochis.test
set.seed(1)
# One-sample test example with normally distributed data
data <- abs(rnorm(10))
mochis.test(data, p = 2, wList = rep(1,10), alternative = "two.sided", approx = "resample")
# Two-sample test with specified weights using normally distributed data
group1 <- rnorm(120, sd = 1)
group2 <- rnorm(120, sd = 2) # Different mean
data <- c(group1, group2)
labels <- c(rep("Group1", 120), rep("Group2", 120))
mochis.test(samples = data, labels = labels, p = 1, wList = c(60:0,1:60), alternative = "two.sided", approx = "resample")
# Two-sample test with automatically estimated weights from NB model
group1 <- countreg::rzinbinom(120, size = 2, mu = 20, pi = 0)
group2 <- countreg::rzinbinom(100, size = 2, mu = 30, pi = 0) # Different mean
data <- c(group1, group2)
labels <- c(rep("Group1", 120), rep("Group2", 100))
mochis.test(samples = data, labels = labels, approx = "asymptotic", measure = "mean", zero_inflation = FALSE)
# Two-sample test with automatically estimated weights from ZINB model
group1 <- countreg::rzinbinom(100, size = 2, mu = 40, pi = 0.1)
group2 <- countreg::rzinbinom(200, size = 1, mu = 40, pi = 0.1) # Different size and zero-inflation
data <- c(group1, group2)
labels <- c(rep("Group1", 100), rep("Group2", 200))
mochis.test(samples = data, labels = labels, alternative = "two.sided", approx = "asymptotic", measure = "dispersion")
# Three-sample test with automatically estimated weights from NB model
group1 <- countreg::rzinbinom(150, size = 1, mu = 30, pi = 0.1)
group2 <- countreg::rzinbinom(100, size = 2, mu = 30, pi = 0.1)
group3 <- countreg::rzinbinom(30, size = 3, mu = 30, pi = 0.1)
data <- c(group1, group2, group3)
labels <- c(rep("Group1", 150), rep("Group2", 100), rep("Group3", 30))
mochis.test(samples = data, labels = labels, alternative = "two.sided", approx = "asymptotic", measure = "dispersion")
example(mochis.test)
devtools::document()
library(mochisR)
?mochis.test
export(mochis.test)
roxygen2::roxygenise()
roxygen2::export(mochis.test)
roxygen2::roxygenise()
library(mochisR)
?mochis.test
example(mochis.test)
set.seed(1)
# One-sample test example with normally distributed data
data <- abs(rnorm(10))
mochis.test(data, p = 2, wList = rep(1,10), alternative = "two.sided", approx = "resample")
# Two-sample test with specified weights using normally distributed data
group1 <- rnorm(120, sd = 1)
group2 <- rnorm(120, sd = 2) # Different mean
data <- c(group1, group2)
labels <- c(rep("Group1", 120), rep("Group2", 120))
mochis.test(samples = data, labels = labels, p = 1, wList = c(60:0,1:60), alternative = "two.sided", approx = "resample")
# Two-sample test with automatically estimated weights from NB model
group1 <- countreg::rzinbinom(120, size = 2, mu = 20, pi = 0)
group2 <- countreg::rzinbinom(100, size = 2, mu = 30, pi = 0) # Different mean
data <- c(group1, group2)
labels <- c(rep("Group1", 120), rep("Group2", 100))
mochis.test(samples = data, labels = labels, approx = "asymptotic", measure = "mean", zero_inflation = FALSE)
# Two-sample test with automatically estimated weights from ZINB model
group1 <- countreg::rzinbinom(100, size = 2, mu = 40, pi = 0.1)
group2 <- countreg::rzinbinom(200, size = 1, mu = 40, pi = 0.1) # Different size and zero-inflation
data <- c(group1, group2)
labels <- c(rep("Group1", 100), rep("Group2", 200))
mochis.test(samples = data, labels = labels, alternative = "two.sided", approx = "asymptotic", measure = "dispersion")
# Three-sample test with automatically estimated weights from NB model
group1 <- countreg::rzinbinom(150, size = 1, mu = 30, pi = 0.1)
group2 <- countreg::rzinbinom(100, size = 2, mu = 30, pi = 0.1)
group3 <- countreg::rzinbinom(30, size = 3, mu = 30, pi = 0.1)
data <- c(group1, group2, group3)
labels <- c(rep("Group1", 150), rep("Group2", 100), rep("Group3", 30))
mochis.test(samples = data, labels = labels, alternative = "two.sided", approx = "asymptotic", measure = "dispersion")
[Package mochisR version 0.2.0 Index]
